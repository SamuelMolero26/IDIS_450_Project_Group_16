# US Regional Sales Analysis Project

A comprehensive data analysis and machine learning project for US regional sales data. This project performs end-to-end data preprocessing, exploratory data analysis, advanced modeling with meta-learning, and generates enhanced statistical visualizations for sales pattern analysis, inventory optimization, and customer segmentation.

## ğŸ¯ What This Project Does

This project analyzes a dataset of 7,992 US regional sales transactions across multiple sales channels (In-Store, Online, Distributor, and Wholesale). It provides:

- **Automated Data Preprocessing**: Handles missing values, outlier detection, data type conversions, and feature engineering
- **Advanced Modeling Pipeline**: Complete ML pipeline with meta-learning, continuous learning, and intelligent configuration optimization
- **Quantitative & Qualitative Evaluation**: Comprehensive evaluation including bias-variance analysis, SHAP interpretability, and business alignment checks
- **Redis Caching Layer**: High-performance caching with SQLite fallback for experiment metadata and results
- **Statistical Analysis**: Performs comprehensive exploratory data analysis with correlation analysis, distribution analysis, and statistical summaries
- **Enhanced Visualizations**: Generates advanced visualizations including model comparisons, feature importance, and interactive dashboards
- **Version Control System**: Dataset and model versioning with hash-based tracking
- **Self-Improving System**: Continuous learning loop that optimizes configurations from historical experiment data

## ğŸŒŸ Key Features

### Data Processing & Analysis
- **Multi-stage Data Cleaning Pipeline**: Univariate, multivariate, and contextual outlier detection using Z-score, IQR, Isolation Forest, and Local Outlier Factor methods
- **Derived Feature Engineering**: Automatically calculates profit margins, total revenue, and temporal lead time metrics
- **Enhanced Visualizations**: Statistical overlays including mean, median, quartiles, skewness, and kurtosis on distributions
- **Correlation Analysis**: Clustered correlation heatmaps with hierarchical clustering for identifying variable relationships

### Advanced Machine Learning Pipeline
- **Meta-Learning System**: Predicts optimal model configurations from historical experiment data using gradient boosting
- **Continuous Learning Loop**: Self-improving system that adapts strategies and uses warm starts for faster convergence
- **Multiple Model Support**: Linear/Logistic Regression, Decision Trees, and Random Forest implementations with automated hyperparameter tuning
- **Cross-Validation**: K-fold CV with comprehensive bias-variance decomposition using bootstrapping
- **Qualitative Evaluation**: SHAP-based interpretability, error pattern analysis, and business rule validation

### Infrastructure & Optimization
- **Redis Cache Layer**: High-performance caching with automatic SQLite fallback for resilient operation
- **Version Control**: Hash-based dataset and model versioning with archival and comparison capabilities
- **Comprehensive Logging**: Structured experiment logging with detailed performance metrics
- **Interactive Reporting**: JSON-based pipeline reports with experiment tracking and reproducibility

### Visualization Suite
- **ML Analysis Visualizations**: Model comparison charts, feature importance plots, bias-variance analysis, and hyperparameter tuning curves
- **Interactive Dashboards**: Plotly-based interactive model performance dashboards
- **Statistical Visualizations**: Enhanced histograms, scatter plots, and correlation heatmaps with statistical overlays

## ğŸ“‹ Prerequisites

- Python 3.8+ (3.9+ recommended)
- pip package manager
- Redis server (optional, SQLite fallback available)
- 16GB+ RAM recommended for full pipeline
- 50GB+ storage for datasets, models, and cached results

## ğŸš€ Getting Started

### Installation

1. Clone the repository:
```bash
git clone https://github.com/SamuelMolero26/IDIS_450_Project_Group_16.git
cd IDIS_450_Project_Group_16
```

2. Create and activate a virtual environment (recommended):
```bash
# Using conda
conda create -n advanced_pipeline python=3.9
conda activate advanced_pipeline

# Or using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install required dependencies:
```bash
pip install -r requirements.txt
```

4. (Optional) Install and start Redis for optimal caching:
```bash
# macOS with Homebrew
brew install redis
brew services start redis

# Ubuntu/Debian
sudo apt-get install redis-server
sudo systemctl start redis-server

# Note: If Redis is unavailable, the system automatically falls back to SQLite
```

### Usage

#### 1. Data Preprocessing

Run the preprocessing pipeline to clean and prepare the sales data:

```bash
python src/data_preprocessing.py
```

**What it does:**
- Loads raw sales data from `Project4_USRegionalSales/Data-USRegionalSales.csv`
- Performs data quality checks and handles missing values
- Detects and flags outliers using multiple methods
- Creates derived features (profit margins, lead times, total revenue)
- Saves preprocessed data to `preprocessed_sales_data.csv`

**Output:**
- `preprocessed_sales_data.csv`: Cleaned dataset with 16 original + derived features
- Visualizations in `visualizations/preprocessing/`: sales distribution, correlations, etc.

#### 2. Run the Advanced Modeling Pipeline

Execute the complete ML pipeline with meta-learning and continuous improvement:

```bash
python main_pipeline.py
```

**What it does:**
- Loads and versions preprocessed data
- Trains multiple models (Linear Regression, Decision Trees, Random Forest)
- Performs quantitative evaluation with bias-variance analysis
- Conducts qualitative evaluation using SHAP for interpretability
- Applies meta-learning to optimize configurations
- Executes continuous learning loop for self-improvement
- Generates comprehensive reports and visualizations

**Output:**
- Pipeline reports: `reports/pipeline_report_<experiment_id>.json`
- ML visualizations: `visualizations/ml_analysis/`
  - Model comparison charts (comprehensive, radar plots)
  - Feature importance plots
  - Bias-variance analysis
  - Hyperparameter tuning curves
  - Interactive dashboards
- Experiment logs: `logs/pipeline.log`, `logs/model.log`, etc.
- Cached results: `cache/` directory with Redis/SQLite backend

#### 3. Generate Enhanced Statistical Visualizations

Create advanced statistical visualizations for exploratory analysis:

```bash
python utils/improved_visualizations.py
```

**What it generates:**
- Enhanced histograms with KDE, mean/median lines, quartiles, and statistical metrics
- Scatter plots with regression lines, correlation coefficients, and outlier highlighting
- Clustered correlation heatmap with hierarchical clustering
- All visualizations saved in the `visualizations/preprocessing/` directory

**Key Visualizations:**
- Distribution analysis for Order Quantity, Unit Price, Unit Cost, Profit Margin, Total Revenue
- Relationship plots: Unit Price vs Total Revenue, Unit Cost vs Profit Margin
- Statistical overlays showing measures of location, dispersion, and shape

### Example Workflows

#### Exploratory Data Analysis
```python
# Load preprocessed data
import pandas as pd
df = pd.read_csv('preprocessed_sales_data.csv')

# View data summary
print(df.describe())
print(df.columns)

# Analyze profit margins by channel
profit_by_channel = df.groupby('Sales Channel')['Profit_Margin'].agg(['mean', 'median', 'std'])
print(profit_by_channel)

# Calculate total revenue by channel
revenue_by_channel = df.groupby('Sales Channel')['Total_Revenue'].sum()
print(revenue_by_channel)
```

#### Running the Pipeline Programmatically
```python
# Import and run the complete pipeline
from src.main_pipeline import run_standard_pipeline

# Execute the pipeline
result = run_standard_pipeline()

# Access results
print(f"Experiment ID: {result['experiment_id']}")
print(f"Best Model: {result['modeling_results']['best_model']}")
print(f"Performance: {result['modeling_results']['best_performance']}")

# View qualitative insights
shap_insights = result['qualitative_results']['shap_analysis']
error_patterns = result['qualitative_results']['error_analysis']
```

#### Accessing Cached Results
```python
# Use the Redis cache layer
from redis_cache import cache

# Retrieve cached experiment results
experiment_id = "12cc04a2"
cached_result = cache.get(f"experiment:{experiment_id}")

# Get meta-learning recommendations
optimal_config = cache.get("meta_learner:optimal_config")
```

## ğŸ“Š Dataset Information

**Source:** US Regional Sales Data  
**Records:** 7,992 transactions  
**Time Period:** 2017-2018  

**Key Variables:**
- **Order Information**: OrderNumber, OrderDate, ShipDate, DeliveryDate
- **Sales Details**: Sales Channel, Order Quantity, Unit Price, Unit Cost, Discount Applied
- **Identifiers**: SalesTeamID, CustomerID, StoreID, ProductID, WarehouseCode
- **Derived Features**: Profit_Margin, Total_Revenue, Total_Lead_Time, Procurement_to_Order_Days

**Sales Channels:**
- In-Store: Direct retail purchases
- Online: E-commerce transactions
- Distributor: B2B distribution sales
- Wholesale: Bulk sales to retailers

## ğŸ“ Project Structure

```
IDIS_450_Project_Group_16/
â”œâ”€â”€ main_pipeline.py                # Main entry point for advanced modeling pipeline
â”œâ”€â”€ redis_cache.py                  # Redis caching with SQLite fallback
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ preprocessed_sales_data.csv     # Cleaned and preprocessed dataset
â”‚
â”œâ”€â”€ src/                            # Core pipeline modules
â”‚   â”œâ”€â”€ config.py                   # Configuration management
â”‚   â”œâ”€â”€ logger.py                   # Structured logging system
â”‚   â”œâ”€â”€ data_loader.py              # Data loading and versioning
â”‚   â”œâ”€â”€ data_preprocessing.py       # Data cleaning and preprocessing
â”‚   â”œâ”€â”€ model_pipeline.py           # Core ML model training
â”‚   â”œâ”€â”€ evaluation_engine.py        # Quantitative evaluation with bias-variance
â”‚   â”œâ”€â”€ qualitative_evaluator.py    # SHAP analysis and business alignment
â”‚   â”œâ”€â”€ meta_learner.py             # Meta-learning for config optimization
â”‚   â”œâ”€â”€ version_control.py          # Dataset and model versioning
â”‚   â”œâ”€â”€ continuous_learning.py      # Self-improvement cycle
â”‚   â””â”€â”€ main_pipeline.py            # Pipeline orchestrator
â”‚
â”œâ”€â”€ utils/                          # Utility modules
â”‚   â”œâ”€â”€ improved_visualizations.py  # Enhanced statistical visualizations
â”‚   â”œâ”€â”€ model_insights_visualization.py  # ML-specific visualizations
â”‚   â””â”€â”€ visualization_utils.py      # Plotting helper functions
â”‚
â”œâ”€â”€ visualizations/                 # Generated visualization outputs
â”‚   â”œâ”€â”€ preprocessing/              # Data analysis visualizations
â”‚   â”‚   â”œâ”€â”€ enhanced_histogram_*.png
â”‚   â”‚   â”œâ”€â”€ enhanced_scatter_*.png
â”‚   â”‚   â””â”€â”€ clustered_correlation_heatmap.png
â”‚   â””â”€â”€ ml_analysis/                # ML pipeline visualizations
â”‚       â”œâ”€â”€ model_comparison_*.png
â”‚       â”œâ”€â”€ feature_importance.png
â”‚       â”œâ”€â”€ bias_variance_analysis.png
â”‚       â””â”€â”€ interactive_model_dashboard.html
â”‚
â”œâ”€â”€ reports/                        # Pipeline execution reports
â”‚   â””â”€â”€ pipeline_report_*.json      # Comprehensive experiment results
â”‚
â”œâ”€â”€ logs/                           # Structured experiment logs
â”‚   â”œâ”€â”€ pipeline.log
â”‚   â”œâ”€â”€ model.log
â”‚   â”œâ”€â”€ evaluation.log
â”‚   â””â”€â”€ meta.log
â”‚
â”œâ”€â”€ cache/                          # Cached results and metadata
â”‚   â”œâ”€â”€ cache.db                    # SQLite fallback cache
â”‚   â”œâ”€â”€ learning_history.json      # Continuous learning data
â”‚   â””â”€â”€ versions/                   # Dataset and model versions
â”‚
â”œâ”€â”€ Project4_USRegionalSales/
â”‚   â”œâ”€â”€ Data-USRegionalSales.csv   # Raw sales data
â”‚   â””â”€â”€ README.md                   # Project subdirectory readme
â”‚
â””â”€â”€ Documentation/
    â”œâ”€â”€ README.md                   # This file
    â”œâ”€â”€ PIPELINE_IMPLEMENTATION_REPORT.md  # Implementation details
    â”œâ”€â”€ architecture_diagram.md     # System architecture
    â”œâ”€â”€ pipeline_implementation_requirements.md
    â””â”€â”€ visualization_design.md     # Visualization approach
```

## ğŸ” Analysis Capabilities

### Data Preprocessing Features
- **Missing Value Handling**: Imputation strategies based on variable type
- **Outlier Detection**: Z-score, IQR, Isolation Forest, and Local Outlier Factor
- **Feature Engineering**: Temporal differences, profit calculations, revenue metrics
- **Data Validation**: Consistency checks for dates, prices, and quantities
- **Scaling & Encoding**: StandardScaler, MinMaxScaler, and One-Hot Encoding for categorical variables

### Machine Learning Pipeline Features
- **Model Training**: Linear Regression, Decision Trees, Random Forest with automated hyperparameter tuning
- **Cross-Validation**: K-fold CV with stratified splitting and reproducible random seeds
- **Bias-Variance Analysis**: Bootstrapping-based decomposition to understand model behavior
- **Meta-Learning**: Gradient boosting models predict optimal configurations from historical experiments
- **Continuous Learning**: Self-improving system with warm starts and adaptive strategy modification
- **Performance Tracking**: Learning curves, validation curves, and performance trend analysis

### Evaluation Framework
- **Quantitative Metrics**: MSE, RMSE, MAE, RÂ² for regression tasks
- **Qualitative Evaluation**: 
  - SHAP-based feature importance and interpretability
  - Error pattern analysis (systematic failure detection)
  - Business rule validation (domain-specific constraints)
  - Actionable recommendations from qualitative assessment
- **Model Comparison**: Comprehensive comparison across multiple metrics with radar plots

### Visualization Features
- **Distribution Analysis**: Histograms with KDE, statistical overlays (mean, median, mode, quartiles)
- **Statistical Metrics**: Skewness, kurtosis, standard deviation, IQR calculated and displayed
- **Relationship Analysis**: Scatter plots with regression lines, correlation coefficients, and RÂ² values
- **Correlation Networks**: Hierarchical clustering to identify variable groupings
- **ML Visualizations**: Feature importance, bias-variance decomposition, hyperparameter tuning curves
- **Interactive Dashboards**: Plotly-based interactive model performance exploration

### Infrastructure Features
- **Caching**: Redis primary with automatic SQLite fallback for resilient operation
- **Version Control**: Hash-based dataset and model tracking with archival capabilities
- **Logging**: Structured experiment logging with performance metrics and error tracking
- **Reporting**: Comprehensive JSON reports with experiment reproducibility data

## ğŸ“– Documentation

- **Pipeline Implementation Report**: See [PIPELINE_IMPLEMENTATION_REPORT.md](PIPELINE_IMPLEMENTATION_REPORT.md) for comprehensive documentation of the advanced modeling pipeline
- **Architecture Diagram**: See [architecture_diagram.md](architecture_diagram.md) for system architecture and component interactions
- **Implementation Requirements**: See [pipeline_implementation_requirements.md](pipeline_implementation_requirements.md) for infrastructure setup and dependencies
- **Visualization Design**: See [visualization_design.md](visualization_design.md) for detailed explanation of visualization approach and statistical methods
- **Data Variables**: Run `python src/data_preprocessing.py` to view detailed variable descriptions in console output
- **Project Tasks**: See [TODO.txt](TODO.txt) for planned analyses and future enhancements

## ğŸ—ï¸ Advanced Pipeline Architecture

The advanced modeling pipeline follows a modular architecture with these key components:

1. **Data Pipeline**: Loading, versioning, and preprocessing with hash-based change tracking
2. **Core Modeling**: Multiple model types with automated hyperparameter tuning and caching
3. **Enhanced Evaluation**: 
   - Quantitative: Cross-validation, bias-variance decomposition, learning curves
   - Qualitative: SHAP interpretability, error analysis, business alignment
4. **Meta-Learning**: Configuration optimization using gradient boosting on historical data
5. **Continuous Learning**: Self-improvement cycle with warm starts and adaptive strategies
6. **Infrastructure**: Redis caching with SQLite fallback, comprehensive logging, version control

See [architecture_diagram.md](architecture_diagram.md) for detailed component diagrams and data flow.

## ğŸš€ Performance Characteristics

### Computational Requirements
- **CPU**: Multi-core CPU (4+ cores recommended) for parallel CV and bootstrapping
- **Memory**: 16GB+ RAM for in-memory processing and caching
- **Storage**: 50GB+ for datasets, models, and cached results
- **Redis**: 2GB+ allocated for caching (optional, falls back to SQLite)

### Scalability
- **Dataset Size**: Efficiently handles current dataset (8K rows)
- **Model Complexity**: Supports ensemble methods and deep decision trees
- **Caching**: Redis provides sub-millisecond access to historical results
- **Parallelization**: CV folds and bootstrapping iterations can be parallelized

### Key Metrics
- **Pipeline Runtime**: ~5-15 minutes for complete pipeline (depending on configuration)
- **Cache Hit Rate**: 70-90% on subsequent runs with similar configurations
- **Meta-Learning Accuracy**: Improves configuration prediction over time with more experiments
- **Self-Improvement**: Continuous learning shows measurable performance gains after 5-10 iterations

## ğŸ¤ Contributing

This project implements an advanced machine learning pipeline with meta-learning and continuous improvement. Contributions are welcome in the following areas:

- Additional model types (e.g., XGBoost, neural networks)
- Enhanced meta-learning strategies
- Distributed computing support (Dask/Ray)
- Advanced interpretability methods
- Business-specific validation rules
- Performance optimizations

## ğŸ“ Citation

If you use this project in your research or work, please cite:

```
US Regional Sales Analysis with Advanced Modeling Pipeline
IDIS 450 Project Group 16
https://github.com/SamuelMolero26/IDIS_450_Project_Group_16
```

